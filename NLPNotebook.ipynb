{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__dataset is split into train and test, with 11000 tweets in training set and the rest in test dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=pd.read_csv(\"final_dataset.csv\")\n",
    "\n",
    "# Create a 50|50 dataset to properly evaluate models 3009 non none allowed 3009 none\n",
    "\n",
    "test_data = df[11000:].copy()\n",
    "data = df[:11000].copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_processing(raw_tweet):\n",
    "    letters_only=re.sub(\"[^a-zA-Z]\",\" \",raw_tweet)\n",
    "    words=letters_only.lower().split()\n",
    "    stops=set(stopwords.words(\"english\"))\n",
    "    m_w=[w for w in words if not w in stops]\n",
    "    return (\" \".join(m_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tweets=data[\"Tweets\"].size\n",
    "clean_tweet=[]\n",
    "for i in range(0,num_tweets):\n",
    "    clean_tweet.append(tweet_processing(data[\"Tweets\"][i]))\n",
    "data[\"Tweets\"]=clean_tweet \n",
    "\n",
    "\n",
    "num_tweets_test=test_data[\"Tweets\"].size\n",
    "clean_tweet_test=[]\n",
    "for i in range(num_tweets,num_tweets+num_tweets_test):\n",
    "    clean_tweet_test.append(tweet_processing(test_data[\"Tweets\"][i]))\n",
    "test_data[\"Tweets\"]=clean_tweet_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model : SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test_svm, Y_train, Y_test_svm = train_test_split(df.Tweets, df.Class, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer(analyzer = \"word\",tokenizer = None,preprocessor = None,stop_words = None,max_features = 5000)\n",
    "\n",
    "train_data_features=vectorizer.fit_transform(X_train)\n",
    "train_data_features=train_data_features.toarray()\n",
    "\n",
    "test_data_features=vectorizer.transform(X_test_svm)\n",
    "test_data_features=test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Testing\n",
      "Accuracy:  0.8613686534216336\n"
     ]
    }
   ],
   "source": [
    "#SVM with linear kernel\n",
    "clf=svm.SVC(kernel='linear',C=1.0)\n",
    "print (\"Training\")\n",
    "clf.fit(train_data_features,Y_train)\n",
    "\n",
    "print (\"Testing\")\n",
    "predicted=clf.predict(test_data_features)\n",
    "accuracy=np.mean(predicted==Y_test_svm)\n",
    "print (\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__precision,recall,F1 score for svm model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.857795974460375, 0.8613686534216336, 0.8589104290311691, None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "score_svm=precision_recall_fscore_support(Y_test_svm, predicted, average='weighted')\n",
    "print(score_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__creating one hot vector for classes(labels).__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sexism']=0\n",
    "data['racism']=0\n",
    "data['none']=0\n",
    "\n",
    "data['sexism'] = np.where(data['Class'] == 'sexism', 1, 0)\n",
    "data['racism'] = np.where(data['Class'] == 'racism', 1, 0)\n",
    "data['none'] = np.where(data['Class'] == 'none', 1, 0)\n",
    "#data.head()\n",
    "columns=['sexism','racism','none']\n",
    "y=data[columns].values\n",
    "#print(y.shape)\n",
    "\n",
    "\n",
    "test_data['sexism']=0\n",
    "test_data['racism']=0\n",
    "test_data['none']=0\n",
    "\n",
    "test_data['sexism'] = np.where(test_data['Class'] == 'sexism', 1, 0)\n",
    "test_data['racism'] = np.where(test_data['Class'] == 'racism', 1, 0)\n",
    "test_data['none'] = np.where(test_data['Class'] == 'none', 1, 0)\n",
    "#data.head()\n",
    "columns=['sexism','racism','none']\n",
    "y_test=test_data[columns].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenizing words__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "#text1 = \"It's true that the chicken was the best bamboozler in the known multiverse.\"\n",
    "#tokens = word_tokenize(data['Tweets'])\n",
    "data['tokenized_sents'] = data.apply(lambda column: word_tokenize(column['Tweets']), axis=1)\n",
    "test_data['tokenized_sents'] = test_data.apply(lambda column: word_tokenize(column['Tweets']), axis=1)\n",
    "df['tokenized_sents'] = df.apply(lambda column: word_tokenize(column['Tweets']), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating word embeddings__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# define training data\n",
    "sentences = df['tokenized_sents']\n",
    "# train model\n",
    "model = Word2Vec(sentences,window =4,min_count=1,sg=1)\n",
    "#print(model)\n",
    "words = list(model.wv.index_to_key)\n",
    "#print(len(words))\n",
    "model.save('model.bin')\n",
    "\n",
    "#model = Word2Vec.load('model.bin')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating Bilingual Word Embeddings__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21899\n"
     ]
    }
   ],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__creating embedding matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(words)\n",
    "embedding_matrix = np.zeros((vocab_size,100))\n",
    "\n",
    "for i in range(0,len(words)):\n",
    "    embedding_vector = model.wv[words[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "embedding_matrix[vocab_size-1]= np.random.normal(scale=0.6, size=(100, ))\n",
    "    \n",
    "#print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using Keras to train LSTM model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__resizing each tweet to size 50__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['tokenized_sents'])\n",
    "sequences = tokenizer.texts_to_sequences(data['tokenized_sents'])\n",
    "X_t = pad_sequences(sequences, maxlen=50)\n",
    "#print(vocab_size)\n",
    "tokenizer.fit_on_texts(test_data['tokenized_sents'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['tokenized_sents'])\n",
    "X_test = pad_sequences(test_sequences, maxlen=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__calculating a user's tendency towards racism, sexism, neutrality by taking the ratio of number of tweets marked as a particular label and total number of tweets of that user. This is done for each label(sexism, racism and neutral)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[949380854 297877558 272704749 ... 289846547  29424561 289846547]\n"
     ]
    }
   ],
   "source": [
    "count1=[]\n",
    "print(data['User Id'].values)\n",
    "for i in data['User Id'].unique():\n",
    "    #print(i)\n",
    "    count1.append((data['User Id']== i).sum())\n",
    "#print(count1)\n",
    "neutral_count=[]\n",
    "sexism_count=[]\n",
    "racism_count=[]\n",
    "for i in data['User Id'].unique():\n",
    "    neutral_count.append(len(data[(data['User Id']==i) & (data['Class']=='none')]))\n",
    "    sexism_count.append(len(data[(data['User Id']==i) & (data['Class']=='sexism')]))\n",
    "    racism_count.append(len(data[(data['User Id']==i) & (data['Class']=='racism')])) \n",
    "    \n",
    "count_test=[]\n",
    "#print(data['User Id'].values)\n",
    "for i in test_data['User Id'].unique():\n",
    "    #print(i)\n",
    "    count_test.append((test_data['User Id']== i).sum())\n",
    "#print(count1)\n",
    "neutral_count_test=[]\n",
    "sexism_count_test=[]\n",
    "racism_count_test=[]\n",
    "for i in test_data['User Id'].unique():\n",
    "    #neutral_count.append((data['User Id']==i) & (data['Class']== 'none').sum())\n",
    "    neutral_count_test.append(len(test_data[(test_data['User Id']==i) & (test_data['Class']=='none')]))\n",
    "    sexism_count_test.append(len(test_data[(test_data['User Id']==i) & (test_data['Class']=='sexism')]))\n",
    "    racism_count_test.append(len(test_data[(test_data['User Id']==i) & (test_data['Class']=='racism')])) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_len = len(data['User Id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_sexism=[]\n",
    "ratio_neutral=[]\n",
    "ratio_racism=[]\n",
    "\n",
    "for i in range(0,X_len):\n",
    "    ratio_sexism.append(sexism_count[i]/count1[i])\n",
    "    ratio_racism.append(racism_count[i]/count1[i])\n",
    "    ratio_neutral.append(neutral_count[i]/count1[i])\n",
    "        \n",
    "#make a column for ratio of eachc class\n",
    "\n",
    "ratio_sexism_test=[]\n",
    "ratio_neutral_test=[]\n",
    "ratio_racism_test=[]\n",
    "\n",
    "for i in range(0,len(test_data['User Id'].unique())):\n",
    "    ratio_sexism_test.append(sexism_count_test[i]/count_test[i])\n",
    "    ratio_racism_test.append(racism_count_test[i]/count_test[i])\n",
    "    ratio_neutral_test.append(neutral_count_test[i]/count_test[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "for i in data['User Id'].unique():\n",
    "    \n",
    "    #print(i)\n",
    "    data.loc[data['User Id'] == i,'tendency_sexual'] = ratio_sexism[j]\n",
    "    data.loc[data['User Id'] == i,'tendency_racism'] = ratio_racism[j]\n",
    "    data.loc[data['User Id'] == i,'tendency_neutral'] = ratio_neutral[j]\n",
    "    j=j+1\n",
    "\n",
    "k=0\n",
    "for i in test_data['User Id'].unique():\n",
    "    \n",
    "    #print(i)\n",
    "    test_data.loc[test_data['User Id'] == i,'tendency_sexual'] = ratio_sexism_test[k]\n",
    "    test_data.loc[test_data['User Id'] == i,'tendency_racism'] = ratio_racism_test[k]\n",
    "    test_data.loc[test_data['User Id'] == i,'tendency_neutral'] = ratio_neutral_test[k]\n",
    "    k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11000, 1)\n"
     ]
    }
   ],
   "source": [
    "tendency_sexism=data['tendency_sexual'].to_numpy()\n",
    "tendency_racism=data['tendency_racism'].to_numpy()\n",
    "tendency_neutral=data['tendency_neutral'].to_numpy()\n",
    "tendency_sexism=tendency_sexism.reshape(len(tendency_sexism),1)\n",
    "tendency_racism=tendency_sexism.reshape(len(tendency_racism),1)\n",
    "tendency_neutral=tendency_sexism.reshape(len(tendency_neutral),1)\n",
    "print(tendency_sexism.shape)\n",
    "\n",
    "tendency_sexism_test=test_data['tendency_sexual'].to_numpy()\n",
    "tendency_racism_test=test_data['tendency_racism'].to_numpy()\n",
    "tendency_neutral_test=test_data['tendency_neutral'].to_numpy()\n",
    "tendency_sexism_test=tendency_sexism_test.reshape(len(tendency_sexism_test),1)\n",
    "tendency_racism_test=tendency_sexism_test.reshape(len(tendency_racism_test),1)\n",
    "tendency_neutral_test=tendency_sexism_test.reshape(len(tendency_neutral_test),1)\n",
    "#print(tendency_sexism.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__concatenating word vectors with tendencies of users calculated above for each label__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11000, 53)\n"
     ]
    }
   ],
   "source": [
    "#print(X_t)\n",
    "X1_t=np.concatenate((X_t, tendency_sexism), axis=1)\n",
    "X1_t=np.concatenate((X1_t, tendency_racism), axis=1)\n",
    "X1_t=np.concatenate((X1_t, tendency_neutral), axis=1)\n",
    "print(X1_t.shape)\n",
    "\n",
    "\n",
    "X1_test=np.concatenate((X_test, tendency_sexism_test), axis=1)\n",
    "X1_test=np.concatenate((X1_test, tendency_racism_test), axis=1)\n",
    "X1_test=np.concatenate((X1_test, tendency_neutral_test), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training LSTM Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "310/310 [==============================] - 72s 206ms/step - loss: 0.3918 - accuracy: 0.7683 - val_loss: 0.2651 - val_accuracy: 0.8791\n",
      "Epoch 2/5\n",
      "310/310 [==============================] - 63s 203ms/step - loss: 0.3021 - accuracy: 0.8748 - val_loss: 0.2903 - val_accuracy: 0.8909\n",
      "Epoch 3/5\n",
      "310/310 [==============================] - 46s 147ms/step - loss: 0.2536 - accuracy: 0.9042 - val_loss: 0.2924 - val_accuracy: 0.8827\n",
      "Epoch 4/5\n",
      "310/310 [==============================] - 35s 114ms/step - loss: 0.1685 - accuracy: 0.9468 - val_loss: 0.6462 - val_accuracy: 0.8845\n",
      "Epoch 5/5\n",
      "310/310 [==============================] - 34s 109ms/step - loss: 0.1243 - accuracy: 0.9620 - val_loss: 0.5316 - val_accuracy: 0.8836\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(53,))\n",
    "x = Embedding(vocab_size,100,weights=[embedding_matrix])(inp)\n",
    "#print(x.values)\n",
    "x = (LSTM(50, return_sequences=True, dropout=0.2, recurrent_dropout=0.4))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50)(x)\n",
    "x=LeakyReLU(alpha=0.02)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(3)(x)\n",
    "x=LeakyReLU(alpha=0.02)(x)\n",
    "model1 = Model(inputs=inp, outputs=x)\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model1.fit(X1_t,y, batch_size=32, epochs=5, validation_split=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model\n",
    "model1.save('model_lstm.h5')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__training a bideirectional LSTM model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "310/310 [==============================] - 33s 93ms/step - loss: 0.4206 - accuracy: 0.7778 - val_loss: 0.4492 - val_accuracy: 0.8827\n",
      "Epoch 2/5\n",
      "310/310 [==============================] - 53s 170ms/step - loss: 0.3493 - accuracy: 0.8295 - val_loss: 0.2382 - val_accuracy: 0.8818\n",
      "Epoch 3/5\n",
      "310/310 [==============================] - 62s 200ms/step - loss: 0.2708 - accuracy: 0.9019 - val_loss: 0.2334 - val_accuracy: 0.8782\n",
      "Epoch 4/5\n",
      "310/310 [==============================] - 60s 194ms/step - loss: 0.3026 - accuracy: 0.8805 - val_loss: 0.3639 - val_accuracy: 0.8500\n",
      "Epoch 5/5\n",
      "310/310 [==============================] - 60s 194ms/step - loss: 0.1888 - accuracy: 0.9399 - val_loss: 0.2715 - val_accuracy: 0.8745\n"
     ]
    }
   ],
   "source": [
    "inp_bi = Input(shape=(53,))\n",
    "x_bi = Embedding(vocab_size,100,weights=[embedding_matrix])(inp_bi)\n",
    "#print(x.values)\n",
    "x_bi = Bidirectional(LSTM(50, return_sequences=True, dropout=0.2, recurrent_dropout=0.4))(x_bi)\n",
    "x_bi = GlobalMaxPool1D()(x_bi)\n",
    "x_bi = Dense(50)(x_bi)\n",
    "x_bi=LeakyReLU(alpha=0.02)(x_bi)\n",
    "x_bi = Dropout(0.2)(x_bi)\n",
    "x_bi = Dense(3)(x_bi)\n",
    "x_bi=LeakyReLU(alpha=0.02)(x_bi)\n",
    "model_bi = Model(inputs=inp_bi, outputs=x_bi)\n",
    "model_bi.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_bi.fit(X1_t,y, batch_size=32, epochs=5, validation_split=0.1);\n",
    "model_bi.save('model_bi.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculating  precision,recall,F1score, of LSTM and bidirectional model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_lstm = model1.predict(X1_test)\n",
    "predicted_bi = model_bi.predict(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_lstm = pd.DataFrame.from_records(predicted_lstm)\n",
    "dataframe_bi=pd.DataFrame.from_records(predicted_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__predicted class is stored in a column of dataframe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.001797</td>\n",
       "      <td>-0.009334</td>\n",
       "      <td>1.102555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.003777</td>\n",
       "      <td>-0.009997</td>\n",
       "      <td>1.262016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.004280</td>\n",
       "      <td>-0.009243</td>\n",
       "      <td>1.189026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.286935</td>\n",
       "      <td>-0.010149</td>\n",
       "      <td>0.736827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.214913</td>\n",
       "      <td>-0.009323</td>\n",
       "      <td>0.706984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0 -0.001797 -0.009334  1.102555\n",
       "1 -0.003777 -0.009997  1.262016\n",
       "2 -0.004280 -0.009243  1.189026\n",
       "3  0.286935 -0.010149  0.736827\n",
       "4  0.214913 -0.009323  0.706984"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_lstm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__bidirectional lstm results__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000487</td>\n",
       "      <td>-0.009390</td>\n",
       "      <td>1.058580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.168027</td>\n",
       "      <td>-0.009005</td>\n",
       "      <td>0.855180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.132320</td>\n",
       "      <td>-0.008515</td>\n",
       "      <td>0.817095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.100366</td>\n",
       "      <td>-0.008921</td>\n",
       "      <td>0.891147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.237748</td>\n",
       "      <td>-0.008313</td>\n",
       "      <td>0.689422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0 -0.000487 -0.009390  1.058580\n",
       "1  0.168027 -0.009005  0.855180\n",
       "2  0.132320 -0.008515  0.817095\n",
       "3  0.100366 -0.008921  0.891147\n",
       "4  0.237748 -0.008313  0.689422"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_bi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__get label function creates appropriate labels according to  predictions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_label(df):\n",
    "    if ((df[0] >df[1]) & (df[0] > df[2])):\n",
    "        return 'Sexism'\n",
    "    elif ((df[1] >df[0]) & (df[1] > df[2])):\n",
    "        return 'Racism'\n",
    "    else:\n",
    "        return 'none'\n",
    "\n",
    "\n",
    "dataframe_lstm['predClass'] = dataframe_lstm.apply(lambda row: get_label(row), axis=1)\n",
    "dataframe_bi['predClass'] = dataframe_bi.apply(lambda row: get_label(row), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__LSTM score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8822377622377623, 0.7661538461538462, 0.8201083423618635, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diego\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Diego\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "score_lstm=precision_recall_fscore_support(test_data['Class'],dataframe_lstm['predClass'],average='weighted')\n",
    "print(score_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bidirectional LSTM score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8868831908831909, 0.7876923076923077, 0.8343500402036987, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diego\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Diego\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "score_bi=precision_recall_fscore_support(test_data['Class'],dataframe_bi['predClass'],average='weighted')\n",
    "print(score_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__precision recall and f1 scores for svm and deep learning models__:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Precision    Recall  F1-score\n",
      "SVM                  0.857796  0.861369  0.858910\n",
      "LSTM                 0.882238  0.766154  0.820108\n",
      "Bidirectional LSTM   0.886883  0.787692  0.834350\n"
     ]
    }
   ],
   "source": [
    "summary = [[score_svm[0],score_svm[1],score_svm[2]], [score_lstm[0],score_lstm[1],score_lstm[2] ],[score_bi[0],score_bi[1],score_bi[2]]]\n",
    "score=pd.DataFrame(summary, columns=[\"Precision\", \"Recall\",\"F1-score\"])\n",
    "score.rename(index={0:'SVM',1:'LSTM',2:'Bidirectional LSTM'}, inplace=True)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
